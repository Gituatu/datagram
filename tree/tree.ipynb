{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyP8C1ICQpLwh6MBKO+ZufF1"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":9,"metadata":{"id":"n8A59AXxDaJ7","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1664261876334,"user_tz":-330,"elapsed":629,"user":{"displayName":"Koushik Saha","userId":"17360145372338622315"}},"outputId":"9196be17-3ef0-48e2-c9f2-0160865f10de"},"outputs":[{"output_type":"stream","name":"stdout","text":["Is diameter >= 3?\n","--> True:\n"," Is color == Yellow?\n"," --> True:\n","  Predict {'Mango': 1, 'Lemon': 1}\n"," --> False:\n","  Predict {'Mango': 1}\n","--> False:\n"," Predict {'Grape': 2}\n","Actual: Mango. Predicted: {'Mango': '100%'}.\n","Actual: Mango. Predicted: {'Mango': '50%', 'Lemon': '50%'}.\n","Actual: Grape. Predicted: {'Grape': '100%'}.\n","Actual: Grape. Predicted: {'Grape': '100%'}.\n","Actual: Lemon. Predicted: {'Mango': '50%', 'Lemon': '50%'}.\n"]}],"source":["#for python 2/3 compatability\n","#from __future__ import print_function\n","\n","#sample dataset\n","# format: each row is an example\n","# The last column is the label\n","# The first two columns are features\n","# If you want you can add more features and examples\n","# Interesting note: 2nd and 5th examples have the same features, but different labels\n","# Let's see how tree handles this case.\n","\n","training_data= [\n","    ['Green',3,'Mango'],['Yellow',3,'Mango'],['Red',1,'Grape'],\n","    ['Red',1,'Grape'],['Yellow',3,'Lemon']\n","]\n","\n","# Column labels\n","# These are used only to print the tree\n","\n","header=[\"color\",\"diameter\",\"label\"]\n","\n","def unique_vals(rows,col):\n","  \"\"\"Find the unique values for a column in a dataset.\"\"\"\n","  return set(row[col]for row in rows)\n","\n","# Demo:\n","# unique_vals(training_data,0)\n","# unique_vals(training_data,1)\n","\n","def class_counts(rows):\n","  \"\"\"Counts the number of each type of example in a dataset.\"\"\"\n","  counts= {} # a dictionary of label -> count.\n","  for row in rows:\n","    # in our dataset format, the label is always the lost column.\n","    label= row[-1]\n","    if label not in counts:\n","      counts[label]= 0\n","    counts[label] +=1\n","  return counts\n","\n","# Demo:\n","# class_counts(training_data)\n","\n","def is_numeric(value):\n","  \"\"\"Test if a value is numeric\"\"\"\n","  return isinstance(value, int) or isinstance(value, float)\n","\n","# Demo:\n","# is_numeric(7)\n","# is_numeric(\"Red\")\n","\n","class Question:\n","  \"\"\"A question is used to partition a dataset.\n","  This class just records a 'column number' (e.g., 0 for Color) and a\n","  'column value' (e.g., Green). The 'match' method is used to compare\n","  the feature value in an example to the feature value stored in the\n","  question. see the demo:\"\"\"\n","\n","  def __init__(self, column, value):\n","    self.column= column\n","    self.value= value\n","\n","  def match(self, example):\n","    # Compare the feature value in an example to the feature value in this question\n","    val= example[self.column]\n","    if is_numeric(val):\n","      return val>=self.value\n","    else:\n","      return val == self.value\n","\n","  def __repr__(self):\n","    # This is just a helper method to print the question in a readable format\n","    condition= \"==\"\n","    if is_numeric(self.value):\n","      condition= \">=\"\n","    return \"Is %s %s %s?\" % (\n","        header[self.column], condition, str(self.value)\n","    )\n","\n","def partition(rows, question):\n","  \"\"\"Partitions the dataset.\n","  For each row in the dataset, check if it matches the question. If so,\n","  and it to 'true rows', otherwise, add it to 'false rows'.\"\"\"\n","  true_rows, false_rows= [], []\n","  for row in rows:\n","    if question.match(row):\n","      true_rows.append(row)\n","    else:\n","      false_rows.append(row)\n","  return true_rows, false_rows\n","\n","# Demo:\n","# Let's partition the training data based on whether rows are Red.\n","# true_rows, false_rows= partition(training_data, Question(0, 'Red'))\n","# This will contain all Red rows.\n","# true_rows\n","# This will contain everything else.\n","# false_rows\n","\n","def gini(rows):\n","  \"\"\"Calculate the gini impurity for a list of rows.\"\"\"\n","  counts= class_counts(rows)\n","  impurity=1\n","  for lbl in counts:\n","    prob_of_lbl= counts[lbl]/float(len(rows))\n","    impurity -= prob_of_lbl**2\n","  return impurity\n","\n","# Demo:\n","# Let's look at some example to understand how gini impurity works.\n","# First we'll look at a dataset with no mixing.\n","# no_mixing =[['Mango'],['Mango']]\n","# This will return 0\n","# Lots_of_mixing= [['Mango'],['Orange'],['Grape'],['Bleberry']]\n","# This will return 0.8\n","# gini(Lots_of_mixing)\n","\n","def info_gain(left, right, current_uncertainty):\n","  \"\"\"Information gain.\n","  The uncertainty of the starting mode, minus the weighted impurity of the child nodes\"\"\"\n","  p= float(len(left)/(len(left)+len(right)))\n","  return current_uncertainty-p*gini(left)-(1-p)*gini(right)\n","\n","def find_best_split(rows):\n","  \"\"\"Find the best question to ask by iterating over every feature / value\n","  and calculating the info gain.\"\"\"\n","  best_gain= 0 # keep track of the best info gain\n","  best_question= None # keep train of the feature / value that producd it\n","  current_uncertainty= gini(rows)\n","  n_features= len(rows[0])-1 # number of columns\n","\n","  for col in range(n_features): # for each feature\n","    values= set([row[col] for row in rows]) # unique values in the column\n","    for val in values: # for each value\n","      question= Question(col, val)\n","    # try splitting the dataset\n","      true_rows, false_rows= partition(rows, question)\n","    # skip this split if it doesn't divide the dataset.\n","      if len(true_rows) == 0 or len(false_rows) == 0:\n","        continue\n","    # Calculate the info gain from this split\n","      gain= info_gain(true_rows, false_rows, current_uncertainty)\n","\n","    # You actually can use '>' instead of '>=' here but i wanted the tree to look a certain way for our dataset.\n","      if gain >= best_gain:\n","        best_gain, best_question= gain, question\n","  return best_gain, best_question\n","\n","class Leaf:\n","  def __init__(self,rows):\n","    self.predictions= class_counts(rows)\n","\n","class Decision_node:\n","  def __init__(self, question, true_branch, false_branch):\n","    self.question= question\n","    self.true_branch= true_branch\n","    self.false_branch= false_branch\n","\n","def build_tree(rows):\n","  gain, question= find_best_split(rows)\n","  if gain == 0:\n","    return Leaf(rows)\n","  true_rows, false_rows= partition(rows, question)\n","  true_branch= build_tree(true_rows)\n","  false_branch= build_tree(false_rows)\n","\n","  return Decision_node(question, true_branch, false_branch)\n","\n","def print_tree(node, spacing=\"\"):\n","  if isinstance(node, Leaf):\n","    print(spacing + \"Predict\", node.predictions)\n","    return\n","  print(spacing + str(node.question))\n","  print(spacing + '--> True:')\n","  print_tree(node.true_branch, spacing + \" \")\n","  print(spacing + '--> False:')\n","  print_tree(node.false_branch, spacing + \" \")\n","\n","def classify(row, node):\n","  if isinstance(node, Leaf):\n","    return node.predictions\n","\n","  if node.question.match(row):\n","    return classify(row, node.true_branch)\n","  else:\n","    return classify(row, node.false_branch)\n","\n","def print_leaf(counts):\n","  total= sum(counts.values())*1.0\n","  probs={}\n","  for lbl in counts.keys():\n","    probs[lbl]= str(int(counts[lbl]/total*100))+ \"%\"\n","  return probs\n","\n","if __name__=='__main__':\n","  my_tree = build_tree(training_data)\n","  print_tree(my_tree)\n","\n","  testing_data=[['Green',3,'Mango'],['Yellow',4,'Mango'],['Red',2,'Grape'],\n","    ['Red',1,'Grape'],['Yellow',3,'Lemon']]\n","\n","  for row in testing_data:\n","    print(\"Actual: %s. Predicted: %s.\" %\n","          (row[-1], print_leaf(classify(row, my_tree))))"]}]}